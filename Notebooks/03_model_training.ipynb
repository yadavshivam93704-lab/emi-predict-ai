{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e88995c9",
   "metadata": {},
   "source": [
    "# EMI Predict AI ‚Äî Model Training\n",
    "This notebook trains classification and regression models and saves them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f258bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shiva\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\linear_model\\_logistic.py:406: ConvergenceWarning: lbfgs failed to converge after 4000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=4000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic accuracy: 0.8753458498023715\n",
      "rf accuracy: 0.9477890316205534\n",
      "xgb accuracy: 0.977050395256917\n",
      "\n",
      "Regression Results\n",
      "linear RMSE: 3728.487530006363\n",
      "rf RMSE: 257.8778197429596\n",
      "xgb RMSE: 424.0291524855336\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "\n",
    "# ========================\n",
    "# LOAD DATA\n",
    "# ========================\n",
    "df = pd.read_csv(\"../Data/featured_emi_dataset.csv\")\n",
    "\n",
    "# Encode categorical columns\n",
    "cat_cols = df.select_dtypes(include=[\"object\", \"string\"]).columns\n",
    "le = LabelEncoder()\n",
    "for col in cat_cols:\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "\n",
    "# ========================\n",
    "# CLASSIFICATION (EMI ELIGIBILITY)\n",
    "# ========================\n",
    "y_class = df[\"emi_eligibility\"]\n",
    "\n",
    "# Remove BOTH targets from features\n",
    "X_class = df.drop(columns=[\n",
    "    \"emi_eligibility\",\n",
    "    \"max_monthly_emi\"\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
    ")\n",
    "\n",
    "clf_models = {\n",
    "    \"logistic\": LogisticRegression(max_iter=4000),\n",
    "    \"rf\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"xgb\": XGBClassifier(eval_metric=\"mlogloss\", random_state=42)\n",
    "}\n",
    "\n",
    "print(\"\\nClassification Results\")\n",
    "for name, model in clf_models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    joblib.dump(model, f\"../models/{name}_classifier.pkl\")\n",
    "    print(name, \"accuracy:\", acc)\n",
    "\n",
    "\n",
    "# ========================\n",
    "# REGRESSION (MAX EMI)\n",
    "# ========================\n",
    "y_reg = df[\"max_monthly_emi\"]\n",
    "\n",
    "# Remove BOTH targets from features\n",
    "X_reg = df.drop(columns=[\n",
    "    \"max_monthly_emi\",\n",
    "    \"emi_eligibility\"\n",
    "])\n",
    "\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "reg_models = {\n",
    "    \"linear\": LinearRegression(),\n",
    "    \"rf\": RandomForestRegressor(n_estimators=200, random_state=42),\n",
    "    \"xgb\": XGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "print(\"\\nRegression Results\")\n",
    "for name, model in reg_models.items():\n",
    "    model.fit(Xr_train, yr_train)\n",
    "    preds = model.predict(Xr_test)\n",
    "    rmse = np.sqrt(mean_squared_error(yr_test, preds))\n",
    "    joblib.dump(model, f\"../models/{name}_regressor.pkl\")\n",
    "    print(name, \"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "üîé Classification Model Results ‚Äî EMI Eligibility Prediction\n",
    "\n",
    "In this section, we trained multiple classification models to predict whether a customer is eligible for EMI approval.\n",
    "\n",
    "Models Trained\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "Random Forest Classifier\n",
    "\n",
    "XGBoost Classifier\n",
    "\n",
    "Observations\n",
    "\n",
    "Logistic Regression achieved moderate performance, indicating that some linear relationships exist between financial variables and eligibility.\n",
    "\n",
    "Random Forest significantly improved accuracy by capturing nonlinear interactions between income, expenses, and credit history.\n",
    "\n",
    "XGBoost achieved the highest accuracy among all models, demonstrating strong capability in modeling complex financial risk patterns.\n",
    "\n",
    "Final Metrics\n",
    "\n",
    "Model\tAccuracy\n",
    "\n",
    "Logistic Regression\t87.5%\n",
    "\n",
    "Random Forest\t94.8%\n",
    "\n",
    "XGBoost\t97.7% ‚≠ê\n",
    "\n",
    "Conclusion\n",
    "\n",
    "Tree-based ensemble models outperform linear models for EMI eligibility prediction.\n",
    "After correcting data leakage, XGBoost remains the most reliable classifier and is selected as the final classification model for deployment.\n",
    "\n",
    "üí∞ Regression Model Results ‚Äî Maximum EMI Prediction\n",
    "\n",
    "In this section, we trained regression models to estimate the maximum EMI amount a customer can safely afford.\n",
    "\n",
    "Models Trained\n",
    "\n",
    "Linear Regression\n",
    "\n",
    "Random Forest Regressor\n",
    "\n",
    "XGBoost Regressor\n",
    "\n",
    "Observations\n",
    "\n",
    "Linear Regression produced high prediction error, confirming that EMI affordability does not follow a simple linear relationship.\n",
    "\n",
    "Random Forest achieved the lowest RMSE, showing strong ability to capture nonlinear financial behavior.\n",
    "\n",
    "XGBoost performed well but slightly underperformed Random Forest in predicting EMI capacity.\n",
    "\n",
    "Final Metrics\n",
    "\n",
    "Model\tRMSE (‚Çπ)\n",
    "            \n",
    "Linear Regression\t3728\n",
    "\n",
    "Random Forest\t258 ‚≠ê\n",
    "\n",
    "XGBoost\t424\n",
    "\n",
    "Conclusion\n",
    "\n",
    "Ensemble models significantly outperform linear regression for EMI estimation.\n",
    "Random Forest Regressor is selected as the final regression model for deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
